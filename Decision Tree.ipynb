{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6c1740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHWETANK VERMA\\Anaconda3\\latest\\python.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHWETANK\n",
      "[nltk_data]     VERMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\SHWETANK\n",
      "[nltk_data]     VERMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# !pip install ftfy\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import exp\n",
    "from numpy import sign\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from PIL import Image # getting images in notebook\n",
    "# !pip install gensim\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# !pip install tensorflow_hub\n",
    "\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77992363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414ac5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\Datasets\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2966d83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>train_pid_2811</td>\n",
       "      <td>suffering from low energy : I'm not exaggerati...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8667</th>\n",
       "      <td>train_pid_8668</td>\n",
       "      <td>I feel like perhaps I'm too weak to even exist...</td>\n",
       "      <td>severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>train_pid_767</td>\n",
       "      <td>Depressed 14F and idk how to tell my dad : Whe...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  \\\n",
       "2810  train_pid_2811  suffering from low energy : I'm not exaggerati...   \n",
       "8667  train_pid_8668  I feel like perhaps I'm too weak to even exist...   \n",
       "766    train_pid_767  Depressed 14F and idk how to tell my dad : Whe...   \n",
       "\n",
       "         Label  \n",
       "2810  moderate  \n",
       "8667    severe  \n",
       "766   moderate  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee79241",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\Datasets\\dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46be5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 8891 rows and 3 columns.\n",
      "Test set has 4496 rows and 3 columns.\n",
      "\n",
      "Index(['PID', 'Text_data', 'Label'], dtype='object')\n",
      "Index(['PID', 'Text data', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} rows and {} columns.\".format(train.shape[0], train.shape[1]))\n",
    "print(\"Test set has {} rows and {} columns.\".format(test.shape[0], test.shape[1]))\n",
    "\n",
    "print()\n",
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01be3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment = {'moderate': 1,'not depression': 0,'severe':2}\n",
    "train.Label = [Sentiment[item] for item in train.Label]\n",
    "test.Label= [Sentiment[item] for item in test.Label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67dd6395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>train_pid_6092</td>\n",
       "      <td>I hope things get better for you. : This time ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>train_pid_3665</td>\n",
       "      <td>2020 : Well. I'm not as drunk going into the n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>train_pid_2948</td>\n",
       "      <td>I’m struggling. : Today is the first day I’ve ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "6091  train_pid_6092  I hope things get better for you. : This time ...      0\n",
       "3664  train_pid_3665  2020 : Well. I'm not as drunk going into the n...      1\n",
       "2947  train_pid_2948  I’m struggling. : Today is the first day I’ve ...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9804349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>dev_pid_96</td>\n",
       "      <td>Life is just very bland, to an uncomfortable, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>dev_pid_1149</td>\n",
       "      <td>i have a constant feeling of hopelessness loom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>dev_pid_3997</td>\n",
       "      <td>girl best friend stopped talking to me because...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PID                                          Text data  Label\n",
       "95      dev_pid_96  Life is just very bland, to an uncomfortable, ...      1\n",
       "1148  dev_pid_1149  i have a constant feeling of hopelessness loom...      1\n",
       "3996  dev_pid_3997  girl best friend stopped talking to me because...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "633f9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# NLTK Tweet Tokenizer for now\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# clean up text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Copied from other notebooks\n",
    "    \"\"\"\n",
    "    # expand acronyms\n",
    "    \n",
    "    # special characters\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Our Stuff\n",
    "    \"\"\"\n",
    "    # remove numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    \n",
    "    # remove punctuation and special chars (keep '!')\n",
    "    for p in string.punctuation.replace('!', ''):\n",
    "        text = text.replace(p, '')\n",
    "        \n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # tokenize\n",
    "    text = tknzr.tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [w.lower() for w in text if not w in stop_words]\n",
    "    corpus.append(text)\n",
    "    \n",
    "    # join back\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436fb920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>train_pid_6299</td>\n",
       "      <td>happy new year folks i hope find peace mind re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8838</th>\n",
       "      <td>train_pid_8839</td>\n",
       "      <td>is anyone relate symptoms i diagnosed depressi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>train_pid_3356</td>\n",
       "      <td>what helps ideas ive long period depression i ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>train_pid_1443</td>\n",
       "      <td>i guess depression isolation silence avoidance...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4544</th>\n",
       "      <td>train_pid_4545</td>\n",
       "      <td>new year stuff another year ’ even feel differ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>train_pid_5988</td>\n",
       "      <td>college making depression close unbearable ive...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>train_pid_774</td>\n",
       "      <td>what way start new year its funny i find think...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>train_pid_3403</td>\n",
       "      <td>feeling numb okay first post apologies long an...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>train_pid_6087</td>\n",
       "      <td>you matter youre alone just wanted say i hope ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>train_pid_4456</td>\n",
       "      <td>i fucking hate everything my entire school hol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "6298  train_pid_6299  happy new year folks i hope find peace mind re...      0\n",
       "8838  train_pid_8839  is anyone relate symptoms i diagnosed depressi...      2\n",
       "3355  train_pid_3356  what helps ideas ive long period depression i ...      1\n",
       "1442  train_pid_1443  i guess depression isolation silence avoidance...      1\n",
       "4544  train_pid_4545  new year stuff another year ’ even feel differ...      1\n",
       "5987  train_pid_5988  college making depression close unbearable ive...      1\n",
       "773    train_pid_774  what way start new year its funny i find think...      1\n",
       "3402  train_pid_3403  feeling numb okay first post apologies long an...      2\n",
       "6086  train_pid_6087  you matter youre alone just wanted say i hope ...      0\n",
       "4455  train_pid_4456  i fucking hate everything my entire school hol...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train['Text_data'] = train['Text_data'].apply(lambda s: clean_text(s))\n",
    "test['Text data'] = test['Text data'].apply(lambda s: clean_text(s))\n",
    "\n",
    "# see some cleaned data\n",
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fecf27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['Text_data'].to_numpy()\n",
    "word_freq = {}\n",
    "\n",
    "for text in texts:\n",
    "    for word in text.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2542fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13266 unique tokens.\n",
      "Shape of data tensor: (8891, 40)\n",
      "Shape of label tensor: (8891,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % (num_words - 1))\n",
    "\n",
    "# pad \n",
    "data = pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=MAX_SEQUENCE_LENGTH,\n",
    "    padding='post', \n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "labels = train['Label'].to_numpy()\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d075105",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad4b5552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['courageous', 'course', 'courses', 'coursework', 'court', 'courtesy', 'cousin', 'cousindads', 'cousins', 'cousy', 'cover', 'coverage', 'covered', 'covering', 'covers', 'covid', 'cow', 'coward', 'cowardice', 'cowards', 'cower', 'cowered', 'coworker', 'coworkers', 'coz', 'cozy', 'cps', 'cptsd', 'crab', 'crack', 'cracked', 'cracking', 'cracks', 'cradle', 'cradled', 'craft', 'crafts', 'craig', 'craigslist', 'cramps', 'crap', 'crappier', 'crappy', 'crash', 'crashes', 'crashing', 'crave', 'craved', 'craving', 'cravings', 'crawl', 'crawled', 'crawling', 'crazed', 'craziest', 'crazy', 'crazyo', 'cream', 'create', 'created', 'creates', 'creating', 'creation', 'creations', 'creative', 'creativity', 'creator', 'creators', 'creature', 'creatures', 'credit', 'creep', 'creeped', 'creeping', 'creeps', 'creepy', 'crept', 'crevice', 'crib', 'crickets', 'cried', 'cries', 'crieswhy', 'crime', 'crimes', 'criminal', 'cringe', 'cringey', 'cringing', 'cringy', 'cripple', 'crippled', 'cripples', 'crippling', 'cripplingly', 'crisis', 'criteria', 'critic', 'critical', 'criticised']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(train['Text_data'])\n",
    "\n",
    "# print vocabulary\n",
    "print(vectorizer.get_feature_names()[2500:2600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b2979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07b312a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[\"Text_data\"]\n",
    "y = train[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddcf7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6075f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorization = TfidfVectorizer()\n",
    "xv_train = vectorization.fit_transform(x_train)\n",
    "xv_test = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c957e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X_test, model_object):\n",
    "  \n",
    "    # Predicton on test with giniIndex\n",
    "    y_pred = model_object.predict(xv_test)\n",
    "    print(\"Predicted values:\")\n",
    "    print(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e4c9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(y_test, y_pred):\n",
    "      \n",
    "    print(\"Confusion Matrix: \",\n",
    "        confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Accuracy : \",\n",
    "    accuracy_score(y_test,y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c39500ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[1 1 1 ... 1 1 0]\n",
      "Confusion Matrix:  [[ 181  280   14]\n",
      " [  84 1424    8]\n",
      " [   5  161   66]]\n",
      "Accuracy :  75.16869095816465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.38      0.49       475\n",
      "           1       0.76      0.94      0.84      1516\n",
      "           2       0.75      0.28      0.41       232\n",
      "\n",
      "    accuracy                           0.75      2223\n",
      "   macro avg       0.73      0.53      0.58      2223\n",
      "weighted avg       0.74      0.75      0.72      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with gini\n",
    "model_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
    "            random_state = 123,max_depth=10, min_samples_leaf=6)\n",
    "  \n",
    "# Performing training\n",
    "model_gini.fit(xv_train, y_train)\n",
    "\n",
    "# Prediction using gini\n",
    "y_pred_gini = prediction(xv_test, model_gini)\n",
    "cal_accuracy(y_test, y_pred_gini)\n",
    "print(classification_report(y_test,y_pred_gini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98fb973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[1 1 1 ... 1 1 1]\n",
      "Confusion Matrix:  [[ 212  258    5]\n",
      " [  86 1416   14]\n",
      " [  23  121   88]]\n",
      "Accuracy :  77.19298245614034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.45      0.53       475\n",
      "           1       0.79      0.93      0.86      1516\n",
      "           2       0.82      0.38      0.52       232\n",
      "\n",
      "    accuracy                           0.77      2223\n",
      "   macro avg       0.76      0.59      0.64      2223\n",
      "weighted avg       0.76      0.77      0.75      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with entropy\n",
    "model_entropy = DecisionTreeClassifier(\n",
    "            criterion = \"entropy\", random_state = 123,\n",
    "            max_depth = 10, min_samples_leaf = 6)\n",
    "  \n",
    "# Performing training\n",
    "model_entropy.fit(xv_train, y_train)\n",
    "\n",
    "# Prediction using entropy\n",
    "y_pred_entropy = prediction(xv_test, model_entropy)\n",
    "cal_accuracy(y_test, y_pred_entropy)\n",
    "\n",
    "print(classification_report(y_test,y_pred_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc16bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31a70a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e581e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xv_train = vectorizer.fit_transform(x_train)\n",
    "xv_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b42694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[1 1 1 ... 1 1 0]\n",
      "Confusion Matrix:  [[ 169  303    3]\n",
      " [  83 1428    5]\n",
      " [  15  173   44]]\n",
      "Accuracy :  73.8191632928475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.36      0.46       475\n",
      "           1       0.75      0.94      0.84      1516\n",
      "           2       0.85      0.19      0.31       232\n",
      "\n",
      "    accuracy                           0.74      2223\n",
      "   macro avg       0.74      0.50      0.53      2223\n",
      "weighted avg       0.74      0.74      0.70      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with gini\n",
    "model_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
    "            random_state = 123,max_depth=10, min_samples_leaf=6)\n",
    "  \n",
    "# Performing training\n",
    "model_gini.fit(xv_train, y_train)\n",
    "\n",
    "# Prediction using gini\n",
    "y_pred_gini = prediction(xv_test, model_gini)\n",
    "cal_accuracy(y_test, y_pred_gini)\n",
    "print(classification_report(y_test,y_pred_gini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cfe03d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[1 1 1 ... 1 1 1]\n",
      "Confusion Matrix:  [[ 146  322    7]\n",
      " [  82 1427    7]\n",
      " [   3  136   93]]\n",
      "Accuracy :  74.94376968061178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.31      0.41       475\n",
      "           1       0.76      0.94      0.84      1516\n",
      "           2       0.87      0.40      0.55       232\n",
      "\n",
      "    accuracy                           0.75      2223\n",
      "   macro avg       0.75      0.55      0.60      2223\n",
      "weighted avg       0.74      0.75      0.72      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with entropy\n",
    "model_entropy = DecisionTreeClassifier(\n",
    "            criterion = \"entropy\", random_state = 123,\n",
    "            max_depth = 10, min_samples_leaf = 6)\n",
    "  \n",
    "# Performing training\n",
    "model_entropy.fit(xv_train, y_train)\n",
    "\n",
    "# Prediction using entropy\n",
    "y_pred_entropy = prediction(xv_test, model_entropy)\n",
    "cal_accuracy(y_test, y_pred_entropy)\n",
    "\n",
    "print(classification_report(y_test,y_pred_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caa4c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0008acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# model = gensim.models.Word2Vec() \n",
    "# GoogleModel = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\GoogleNews-vectors-negative300.bin', binary=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdb7aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "import gensim\n",
    "X_train, X_test, y_train, y_test = train_test_split (train['Text_data'], train['Label'] , test_size=0.2)\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=100,\n",
    "                                   window=5,\n",
    "                                   min_count=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0d4c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(w2v_model.wv.index_to_key )\n",
    "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train])\n",
    "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b1bdb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36784228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, min_samples_leaf=6, random_state=123)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision tree with gini\n",
    "model_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
    "            random_state = 123,max_depth=10, min_samples_leaf=6)\n",
    "  \n",
    "# Performing training\n",
    "model_gini.fit(X_train_vect_avg, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1c944b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1779"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_vect_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58164895",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 12045 features, but DecisionTreeClassifier is expecting 100 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5724\\1932407939.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prediction using gini\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_vect_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_gini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcal_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5724\\1740854886.py\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(X_test, model_object)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Predicton on test with giniIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxv_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted values:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m    466\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             if issparse(X) and (\n\u001b[0;32m    435\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    401\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 12045 features, but DecisionTreeClassifier is expecting 100 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "# Prediction using gini\n",
    "y_pred = prediction(X_test_vect_avg, model_gini)\n",
    "cal_accuracy(y_test,y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279c810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
