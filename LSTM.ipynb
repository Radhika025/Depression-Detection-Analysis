{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a097946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHWETANK VERMA\\Anaconda3\\latest\\python.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHWETANK\n",
      "[nltk_data]     VERMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\SHWETANK\n",
      "[nltk_data]     VERMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# !pip install ftfy\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import exp\n",
    "from numpy import sign\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from PIL import Image # getting images in notebook\n",
    "# !pip install gensim\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# !pip install tensorflow_hub\n",
    "\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3e050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad0e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\Datasets\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e9aff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>train_pid_8303</td>\n",
       "      <td>Parents, Indian parents... Help [20M] : So, my...</td>\n",
       "      <td>severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8447</th>\n",
       "      <td>train_pid_8448</td>\n",
       "      <td>What can you do? : A successful person cannot ...</td>\n",
       "      <td>severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>train_pid_7348</td>\n",
       "      <td>My friend told me to kms : [removed]</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  \\\n",
       "8302  train_pid_8303  Parents, Indian parents... Help [20M] : So, my...   \n",
       "8447  train_pid_8448  What can you do? : A successful person cannot ...   \n",
       "7347  train_pid_7348               My friend told me to kms : [removed]   \n",
       "\n",
       "               Label  \n",
       "8302          severe  \n",
       "8447          severe  \n",
       "7347  not depression  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b28ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\Datasets\\dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b03dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 8891 rows and 3 columns.\n",
      "Test set has 4496 rows and 3 columns.\n",
      "\n",
      "Index(['PID', 'Text_data', 'Label'], dtype='object')\n",
      "Index(['PID', 'Text data', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} rows and {} columns.\".format(train.shape[0], train.shape[1]))\n",
    "print(\"Test set has {} rows and {} columns.\".format(test.shape[0], test.shape[1]))\n",
    "\n",
    "print()\n",
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bdc55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment = {'moderate': 1,'not depression': 0,'severe':2}\n",
    "train.Label = [Sentiment[item] for item in train.Label]\n",
    "test.Label= [Sentiment[item] for item in test.Label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b453362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>train_pid_1040</td>\n",
       "      <td>Worst day of the year for me : Every year I’m ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>train_pid_3467</td>\n",
       "      <td>How can I open up to my parents : Hi I want to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>train_pid_453</td>\n",
       "      <td>Why the fuck am i said : Why the fuck am i sad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "1039  train_pid_1040  Worst day of the year for me : Every year I’m ...      1\n",
       "3466  train_pid_3467  How can I open up to my parents : Hi I want to...      1\n",
       "452    train_pid_453  Why the fuck am i said : Why the fuck am i sad...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03cfcb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>dev_pid_1116</td>\n",
       "      <td>Anhedonhia is destroying me : Every fucking da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>dev_pid_2025</td>\n",
       "      <td>I felt happy for a day : I felt like no one ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>dev_pid_3657</td>\n",
       "      <td>Anyone Else Wish for A Fatal Disease?? : [remo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PID                                          Text data  Label\n",
       "1115  dev_pid_1116  Anhedonhia is destroying me : Every fucking da...      1\n",
       "2024  dev_pid_2025  I felt happy for a day : I felt like no one ha...      1\n",
       "3656  dev_pid_3657  Anyone Else Wish for A Fatal Disease?? : [remo...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f4aa290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# NLTK Tweet Tokenizer for now\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# clean up text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Copied from other notebooks\n",
    "    \"\"\"\n",
    "    # expand acronyms\n",
    "    \n",
    "    # special characters\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Our Stuff\n",
    "    \"\"\"\n",
    "    # remove numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    \n",
    "    # remove punctuation and special chars (keep '!')\n",
    "    for p in string.punctuation.replace('!', ''):\n",
    "        text = text.replace(p, '')\n",
    "        \n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # tokenize\n",
    "    text = tknzr.tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [w.lower() for w in text if not w in stop_words]\n",
    "    corpus.append(text)\n",
    "    \n",
    "    # join back\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9571b251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>train_pid_2309</td>\n",
       "      <td>worst year depression crisis im happy ended af...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>train_pid_3287</td>\n",
       "      <td>being okay temprorary happiness ive learnt oka...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8195</th>\n",
       "      <td>train_pid_8196</td>\n",
       "      <td>my problem i know sound fucking egotistic text...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>train_pid_5827</td>\n",
       "      <td>a letter daughter depression dear daughter i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>train_pid_2343</td>\n",
       "      <td>less hours i dont feel ready everything especi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>train_pid_2812</td>\n",
       "      <td>fuck titles i ’ depressed i feel fucking worth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>train_pid_3083</td>\n",
       "      <td>i never sad new years yesterday i literally br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>train_pid_895</td>\n",
       "      <td>just exhausted no matter much right amount sle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>train_pid_264</td>\n",
       "      <td>i hate life its everyone fun im sitting alone ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>train_pid_4998</td>\n",
       "      <td>why i like there deep feeling sadness seems po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "2308  train_pid_2309  worst year depression crisis im happy ended af...      1\n",
       "3286  train_pid_3287  being okay temprorary happiness ive learnt oka...      1\n",
       "8195  train_pid_8196  my problem i know sound fucking egotistic text...      2\n",
       "5826  train_pid_5827  a letter daughter depression dear daughter i h...      1\n",
       "2342  train_pid_2343  less hours i dont feel ready everything especi...      1\n",
       "2811  train_pid_2812  fuck titles i ’ depressed i feel fucking worth...      1\n",
       "3082  train_pid_3083  i never sad new years yesterday i literally br...      1\n",
       "894    train_pid_895  just exhausted no matter much right amount sle...      1\n",
       "263    train_pid_264  i hate life its everyone fun im sitting alone ...      1\n",
       "4997  train_pid_4998  why i like there deep feeling sadness seems po...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train['Text_data'] = train['Text_data'].apply(lambda s: clean_text(s))\n",
    "test['Text data'] = test['Text data'].apply(lambda s: clean_text(s))\n",
    "\n",
    "# see some cleaned data\n",
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2bf6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['Text_data'].to_numpy()\n",
    "word_freq = {}\n",
    "\n",
    "for text in texts:\n",
    "    for word in text.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8984c9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13266 unique tokens.\n",
      "Shape of data tensor: (8891, 40)\n",
      "Shape of label tensor: (8891,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % (num_words - 1))\n",
    "\n",
    "# pad \n",
    "data = pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=MAX_SEQUENCE_LENGTH,\n",
    "    padding='post', \n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "labels = train['Label'].to_numpy()\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f2e8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66979361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['courageous', 'course', 'courses', 'coursework', 'court', 'courtesy', 'cousin', 'cousindads', 'cousins', 'cousy', 'cover', 'coverage', 'covered', 'covering', 'covers', 'covid', 'cow', 'coward', 'cowardice', 'cowards', 'cower', 'cowered', 'coworker', 'coworkers', 'coz', 'cozy', 'cps', 'cptsd', 'crab', 'crack', 'cracked', 'cracking', 'cracks', 'cradle', 'cradled', 'craft', 'crafts', 'craig', 'craigslist', 'cramps', 'crap', 'crappier', 'crappy', 'crash', 'crashes', 'crashing', 'crave', 'craved', 'craving', 'cravings', 'crawl', 'crawled', 'crawling', 'crazed', 'craziest', 'crazy', 'crazyo', 'cream', 'create', 'created', 'creates', 'creating', 'creation', 'creations', 'creative', 'creativity', 'creator', 'creators', 'creature', 'creatures', 'credit', 'creep', 'creeped', 'creeping', 'creeps', 'creepy', 'crept', 'crevice', 'crib', 'crickets', 'cried', 'cries', 'crieswhy', 'crime', 'crimes', 'criminal', 'cringe', 'cringey', 'cringing', 'cringy', 'cripple', 'crippled', 'cripples', 'crippling', 'cripplingly', 'crisis', 'criteria', 'critic', 'critical', 'criticised']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(train['Text_data'])\n",
    "\n",
    "# print vocabulary\n",
    "print(vectorizer.get_feature_names()[2500:2600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3292e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[\"Text_data\"]\n",
    "y = train[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e5d235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5021d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "weather_encoded = le.fit_transform(x)\n",
    "\n",
    "\n",
    "label=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3539c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(weather_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff9e06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9ad8484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import tensorflow \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eff9f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "import keras.backend as K #for some advanced functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7821f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13266 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(train['Text_data'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cff7d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (8891, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(train['Text_data'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23ced266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (8891, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(train['Label']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "323f7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8001, 250) (8001, 3)\n",
      "(890, 250) (890, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25da248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "113/113 [==============================] - 135s 1s/step - loss: 0.7302 - accuracy: 0.7163 - val_loss: 0.4715 - val_accuracy: 0.8427\n",
      "Epoch 2/5\n",
      "113/113 [==============================] - 129s 1s/step - loss: 0.3360 - accuracy: 0.8790 - val_loss: 0.3759 - val_accuracy: 0.8777\n",
      "Epoch 3/5\n",
      "113/113 [==============================] - 135s 1s/step - loss: 0.1899 - accuracy: 0.9372 - val_loss: 0.3599 - val_accuracy: 0.8864\n",
      "Epoch 4/5\n",
      "113/113 [==============================] - 133s 1s/step - loss: 0.1527 - accuracy: 0.9517 - val_loss: 0.3805 - val_accuracy: 0.8851\n",
      "Epoch 5/5\n",
      "113/113 [==============================] - 135s 1s/step - loss: 0.0932 - accuracy: 0.9700 - val_loss: 0.4346 - val_accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6224b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 2s 70ms/step - loss: 0.5352 - accuracy: 0.8663\n",
      "Test set\n",
      "  Loss: 0.535\n",
      "  Accuracy: 0.866\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7d340fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,Y_train, Y_test = train_test_split(X,Y, test_size=0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0f5ff29",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23560\\3847154713.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwords_to_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[0;32m    294\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3731438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_vec):\n",
    "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    for line in f:\n",
    "      w_line = line.split()\n",
    "      curr_word = w_line[0]\n",
    "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "  return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a11df8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map = read_glove_vector('glove.twitter.27B.100d.txt')\n",
    "\n",
    "maxLen = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23d64519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(words_to_index)\n",
    "embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "print(embed_vector_len)\n",
    "emb_matrix = np.array([], dtype=np.int64)  \n",
    "emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "for word, index in words_to_index.items():\n",
    "  #print(word,index)\n",
    "  embedding_vector = word_to_vec_map.get(word)\n",
    "  #print(embedding_vector)\n",
    "  if embedding_vector is not None and embedding_vector.size != 0:\n",
    "    emb_matrix[index,:] = embedding_vector\n",
    "  if(index==12430):\n",
    "    break\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e96a7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_rating(input_shape):\n",
    "\n",
    "  X_indices = Input(input_shape)\n",
    "\n",
    "  embeddings = embedding_layer(X_indices)\n",
    "\n",
    "  X = LSTM(128, return_sequences=True)(embeddings)\n",
    "\n",
    "  X = Dropout(0.6)(X)\n",
    "\n",
    "  X = LSTM(128, return_sequences=True)(X)\n",
    "\n",
    "  X = Dropout(0.6)(X)\n",
    "\n",
    "  X = LSTM(128)(X)\n",
    "\n",
    "  X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "  model = Model(inputs=X_indices, outputs=X)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ea630434",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23560\\2560651377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \"\"\"\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[0;32m    387\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "X_train_indices = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "X_train_indices = pad_sequences(X_train_indices, maxlen=250, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b617f2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7112, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3493df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "112/112 [==============================] - 139s 1s/step - loss: 0.5417 - accuracy: 0.6457\n",
      "Epoch 2/15\n",
      "112/112 [==============================] - 142s 1s/step - loss: 0.5024 - accuracy: 0.6727\n",
      "Epoch 3/15\n",
      "112/112 [==============================] - 142s 1s/step - loss: 0.4991 - accuracy: 0.6744\n",
      "Epoch 4/15\n",
      "112/112 [==============================] - 146s 1s/step - loss: 0.4948 - accuracy: 0.6758\n",
      "Epoch 5/15\n",
      "112/112 [==============================] - 145s 1s/step - loss: 0.4932 - accuracy: 0.6774\n",
      "Epoch 6/15\n",
      "112/112 [==============================] - 141s 1s/step - loss: 0.4916 - accuracy: 0.6781\n",
      "Epoch 7/15\n",
      "112/112 [==============================] - 145s 1s/step - loss: 0.4901 - accuracy: 0.6796\n",
      "Epoch 8/15\n",
      "112/112 [==============================] - 127s 1s/step - loss: 0.4877 - accuracy: 0.6807\n",
      "Epoch 9/15\n",
      "112/112 [==============================] - 138s 1s/step - loss: 0.4864 - accuracy: 0.6808\n",
      "Epoch 10/15\n",
      "112/112 [==============================] - 143s 1s/step - loss: 0.4831 - accuracy: 0.6828\n",
      "Epoch 11/15\n",
      "112/112 [==============================] - 140s 1s/step - loss: 0.4829 - accuracy: 0.6838\n",
      "Epoch 12/15\n",
      "112/112 [==============================] - 148s 1s/step - loss: 0.4806 - accuracy: 0.6852\n",
      "Epoch 13/15\n",
      "112/112 [==============================] - 129s 1s/step - loss: 0.4821 - accuracy: 0.6836\n",
      "Epoch 14/15\n",
      "112/112 [==============================] - 147s 1s/step - loss: 0.4786 - accuracy: 0.6864\n",
      "Epoch 15/15\n",
      "112/112 [==============================] - 142s 1s/step - loss: 0.4760 - accuracy: 0.6871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a262118d30>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_indices, Y_train, batch_size=64, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54662389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 6s 91ms/step - loss: 0.6125 - accuracy: 0.6301\n",
      "Test set\n",
      "  Loss: 0.612\n",
      "  Accuracy: 0.630\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "373a04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b79841c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30416f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6f376307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 50)\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 1400\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "embedding_vector_length = 50\n",
    "embedding_matrix_fasttext = np.zeros((max_features + 1, embedding_vector_length))\n",
    "print(embedding_matrix_fasttext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9eeda439",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
    "word_index = token.word_index\n",
    "for word, i in sorted(token.word_index.items(),key=lambda x:x[1]):\n",
    "    if i > (max_features+1):\n",
    "        break\n",
    "    try:\n",
    "        embedding_vector = ft[word] #Reading word's embedding from Glove model for a given word\n",
    "        embedding_matrix_fasttext[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3d1fb618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 50)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_fasttext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ac247187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fff098bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 1400, 50)          250050    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               60400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 310,551\n",
      "Trainable params: 60,501\n",
      "Non-trainable params: 250,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and one dense layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features+1,\n",
    "                    embedding_vector_length, ### 50 here\n",
    "                    weights=[embedding_matrix_fasttext],\n",
    "                    input_length=maxlen, ### 1400 here\n",
    "                    trainable=False))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "92699b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.Text_data.values, train.Label.values, \n",
    "                                                  stratify=train.Label.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ed05ef02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "token.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train_seq = token.texts_to_sequences(X_train)\n",
    "X_test_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "#zero pad the sequences\n",
    "X_train_pad =pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d91342c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d0be0f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "223/223 [==============================] - 1459s 7s/step - loss: 0.3969 - accuracy: 0.6734 - val_loss: 0.3794 - val_accuracy: 0.6751\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 1461s 7s/step - loss: 0.3736 - accuracy: 0.6753 - val_loss: 0.3701 - val_accuracy: 0.6751\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 4223s 19s/step - loss: 0.3729 - accuracy: 0.6753 - val_loss: 0.3704 - val_accuracy: 0.6751\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 1753s 8s/step - loss: 0.3715 - accuracy: 0.6753 - val_loss: 0.3720 - val_accuracy: 0.6751\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 1754s 8s/step - loss: 0.3726 - accuracy: 0.6753 - val_loss: 0.3698 - val_accuracy: 0.6751\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 1749s 8s/step - loss: 0.3726 - accuracy: 0.6753 - val_loss: 0.3727 - val_accuracy: 0.6751\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 6820s 31s/step - loss: 0.3721 - accuracy: 0.6753 - val_loss: 0.3710 - val_accuracy: 0.6751\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 1708s 8s/step - loss: 0.3721 - accuracy: 0.6753 - val_loss: 0.3699 - val_accuracy: 0.6751\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 1686s 8s/step - loss: 0.3697 - accuracy: 0.6753 - val_loss: 0.3738 - val_accuracy: 0.6751\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 1736s 8s/step - loss: 0.3722 - accuracy: 0.6753 - val_loss: 0.3698 - val_accuracy: 0.6751\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train_pad,y_train,\n",
    "          epochs=10,\n",
    "          batch_size=32,          \n",
    "          validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d20e51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 61s 1s/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_pred = model.predict(X_test_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ab003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "05e4a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=test_pred.reshape((test_pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d4692436",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=np.where(test_pred>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6ec33d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[   0,  397,    0],\n",
      "       [   0, 1201,    0],\n",
      "       [   0,  181,    0]], dtype=int64)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       397\n",
      "           1       0.68      1.00      0.81      1201\n",
      "           2       0.00      0.00      0.00       181\n",
      "\n",
      "    accuracy                           0.68      1779\n",
      "   macro avg       0.23      0.33      0.27      1779\n",
      "weighted avg       0.46      0.68      0.54      1779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(confusion_matrix(y_test,test_pred))\n",
    "print(classification_report(y_test,test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2268188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395112f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810ce41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
