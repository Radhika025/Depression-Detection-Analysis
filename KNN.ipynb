{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39928936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHWETANK VERMA\\Anaconda3\\latest\\python.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHWETANK\n",
      "[nltk_data]     VERMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\SHWETANK\n",
      "[nltk_data]     VERMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# !pip install ftfy\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import exp\n",
    "from numpy import sign\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from PIL import Image # getting images in notebook\n",
    "# !pip install gensim\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# !pip install tensorflow_hub\n",
    "\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57872e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81eb83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\Datasets\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e2ae645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>train_pid_2795</td>\n",
       "      <td>I think I can't be happy without a relationshi...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>train_pid_4549</td>\n",
       "      <td>Starting the new year off \"right\" : First day ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>train_pid_7537</td>\n",
       "      <td>Time is just one big blur when you have depres...</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  \\\n",
       "2794  train_pid_2795  I think I can't be happy without a relationshi...   \n",
       "4548  train_pid_4549  Starting the new year off \"right\" : First day ...   \n",
       "7536  train_pid_7537  Time is just one big blur when you have depres...   \n",
       "\n",
       "               Label  \n",
       "2794        moderate  \n",
       "4548        moderate  \n",
       "7536  not depression  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c27e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\Datasets\\dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2421961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 8891 rows and 3 columns.\n",
      "Test set has 4496 rows and 3 columns.\n",
      "\n",
      "Index(['PID', 'Text_data', 'Label'], dtype='object')\n",
      "Index(['PID', 'Text data', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} rows and {} columns.\".format(train.shape[0], train.shape[1]))\n",
    "print(\"Test set has {} rows and {} columns.\".format(test.shape[0], test.shape[1]))\n",
    "\n",
    "print()\n",
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41aa1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment = {'moderate': 1,'not depression': 0,'severe':2}\n",
    "train.Label = [Sentiment[item] for item in train.Label]\n",
    "test.Label= [Sentiment[item] for item in test.Label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79938ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>train_pid_1188</td>\n",
       "      <td>Ayy sausage body is back in the game : Does an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>train_pid_188</td>\n",
       "      <td>Jeez, looking back at the decade is horribly d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>train_pid_6303</td>\n",
       "      <td>Trying to avoid social media. If anyone wants ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "1187  train_pid_1188  Ayy sausage body is back in the game : Does an...      1\n",
       "187    train_pid_188  Jeez, looking back at the decade is horribly d...      1\n",
       "6302  train_pid_6303  Trying to avoid social media. If anyone wants ...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31285c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>dev_pid_3891</td>\n",
       "      <td>Support group : Hi everyone, I’ve just joined ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>dev_pid_609</td>\n",
       "      <td>Lost my sparks for drawing : You know, i've be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>dev_pid_1888</td>\n",
       "      <td>Just need to post so I don’t feel so alone.. :...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PID                                          Text data  Label\n",
       "3890  dev_pid_3891  Support group : Hi everyone, I’ve just joined ...      0\n",
       "608    dev_pid_609  Lost my sparks for drawing : You know, i've be...      1\n",
       "1887  dev_pid_1888  Just need to post so I don’t feel so alone.. :...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f15796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# NLTK Tweet Tokenizer for now\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# clean up text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Copied from other notebooks\n",
    "    \"\"\"\n",
    "    # expand acronyms\n",
    "    \n",
    "    # special characters\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Our Stuff\n",
    "    \"\"\"\n",
    "    # remove numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    \n",
    "    # remove punctuation and special chars (keep '!')\n",
    "    for p in string.punctuation.replace('!', ''):\n",
    "        text = text.replace(p, '')\n",
    "        \n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # tokenize\n",
    "    text = tknzr.tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [w.lower() for w in text if not w in stop_words]\n",
    "    corpus.append(text)\n",
    "    \n",
    "    # join back\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86d9d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8659</th>\n",
       "      <td>train_pid_8660</td>\n",
       "      <td>help im new reddit i dont really know works i ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>train_pid_4996</td>\n",
       "      <td>conflicted i know i go social part wants i als...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>train_pid_1468</td>\n",
       "      <td>how everyone ’ new year oh mine something else...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>train_pid_2201</td>\n",
       "      <td>for last months life falling apart im my girlf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492</th>\n",
       "      <td>train_pid_3493</td>\n",
       "      <td>feeling alone nye party i went first real nye ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>train_pid_1145</td>\n",
       "      <td>i ’ depressed high i need someone life love ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>train_pid_3021</td>\n",
       "      <td>happy new year it new year everyone takes deci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6642</th>\n",
       "      <td>train_pid_6643</td>\n",
       "      <td>how i nonreligious help girlfriend religious g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8827</th>\n",
       "      <td>train_pid_8828</td>\n",
       "      <td>don ’ know what do anymore meds ’ worked thera...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8831</th>\n",
       "      <td>train_pid_8832</td>\n",
       "      <td>what wrong do ever lie wondering actually wron...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "8659  train_pid_8660  help im new reddit i dont really know works i ...      2\n",
       "4995  train_pid_4996  conflicted i know i go social part wants i als...      1\n",
       "1467  train_pid_1468  how everyone ’ new year oh mine something else...      1\n",
       "2200  train_pid_2201  for last months life falling apart im my girlf...      1\n",
       "3492  train_pid_3493  feeling alone nye party i went first real nye ...      1\n",
       "1144  train_pid_1145  i ’ depressed high i need someone life love ev...      1\n",
       "3020  train_pid_3021  happy new year it new year everyone takes deci...      1\n",
       "6642  train_pid_6643  how i nonreligious help girlfriend religious g...      0\n",
       "8827  train_pid_8828  don ’ know what do anymore meds ’ worked thera...      2\n",
       "8831  train_pid_8832  what wrong do ever lie wondering actually wron...      2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train['Text_data'] = train['Text_data'].apply(lambda s: clean_text(s))\n",
    "test['Text data'] = test['Text data'].apply(lambda s: clean_text(s))\n",
    "\n",
    "# see some cleaned data\n",
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8062ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['Text_data'].to_numpy()\n",
    "word_freq = {}\n",
    "\n",
    "for text in texts:\n",
    "    for word in text.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "249db77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13266 unique tokens.\n",
      "Shape of data tensor: (8891, 40)\n",
      "Shape of label tensor: (8891,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % (num_words - 1))\n",
    "\n",
    "# pad \n",
    "data = pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=MAX_SEQUENCE_LENGTH,\n",
    "    padding='post', \n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "labels = train['Label'].to_numpy()\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc527d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e31c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['courageous', 'course', 'courses', 'coursework', 'court', 'courtesy', 'cousin', 'cousindads', 'cousins', 'cousy', 'cover', 'coverage', 'covered', 'covering', 'covers', 'covid', 'cow', 'coward', 'cowardice', 'cowards', 'cower', 'cowered', 'coworker', 'coworkers', 'coz', 'cozy', 'cps', 'cptsd', 'crab', 'crack', 'cracked', 'cracking', 'cracks', 'cradle', 'cradled', 'craft', 'crafts', 'craig', 'craigslist', 'cramps', 'crap', 'crappier', 'crappy', 'crash', 'crashes', 'crashing', 'crave', 'craved', 'craving', 'cravings', 'crawl', 'crawled', 'crawling', 'crazed', 'craziest', 'crazy', 'crazyo', 'cream', 'create', 'created', 'creates', 'creating', 'creation', 'creations', 'creative', 'creativity', 'creator', 'creators', 'creature', 'creatures', 'credit', 'creep', 'creeped', 'creeping', 'creeps', 'creepy', 'crept', 'crevice', 'crib', 'crickets', 'cried', 'cries', 'crieswhy', 'crime', 'crimes', 'criminal', 'cringe', 'cringey', 'cringing', 'cringy', 'cripple', 'crippled', 'cripples', 'crippling', 'cripplingly', 'crisis', 'criteria', 'critic', 'critical', 'criticised']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(train['Text_data'])\n",
    "\n",
    "# print vocabulary\n",
    "print(vectorizer.get_feature_names()[2500:2600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41f1342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[\"Text_data\"]\n",
    "y = train[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3615e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d2a9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "weather_encoded = le.fit_transform(x)\n",
    "\n",
    "\n",
    "label=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa304c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(weather_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "301f3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9008d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "668b070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorization = TfidfVectorizer()\n",
    "xv_train = vectorization.fit_transform(x_train)\n",
    "xv_test = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec956ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b825bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = knn.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbebc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X_test, model_object):\n",
    "    y_pred = model_object.predict(xv_test)\n",
    "    print(\"Predicted values:\")\n",
    "    print(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fd476de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(y_test, y_pred):\n",
    "      \n",
    "    print(\"Confusion Matrix: \",\n",
    "        confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Accuracy : \",\n",
    "    accuracy_score(y_test,y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd21b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[0 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = prediction(xv_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9157be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:  [[ 298  176   19]\n",
      " [  46 1419   18]\n",
      " [  11   96  140]]\n",
      "Accuracy :  83.53576248313091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.60      0.70       493\n",
      "           1       0.84      0.96      0.89      1483\n",
      "           2       0.79      0.57      0.66       247\n",
      "\n",
      "    accuracy                           0.84      2223\n",
      "   macro avg       0.82      0.71      0.75      2223\n",
      "weighted avg       0.83      0.84      0.83      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cal_accuracy(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81b670c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using count vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "xv_train = vectorizer.fit_transform(x_train)\n",
    "xv_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38223e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[0 0 1 ... 1 1 0]\n",
      "Confusion Matrix:  [[ 352  131   10]\n",
      " [ 103 1357   23]\n",
      " [  33   75  139]]\n",
      "Accuracy :  83.13090418353576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72       493\n",
      "           1       0.87      0.92      0.89      1483\n",
      "           2       0.81      0.56      0.66       247\n",
      "\n",
      "    accuracy                           0.83      2223\n",
      "   macro avg       0.80      0.73      0.76      2223\n",
      "weighted avg       0.83      0.83      0.83      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = knn.fit(xv_train, y_train)\n",
    "y_pred = prediction(xv_test, clf)\n",
    "cal_accuracy(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db2cfc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pretained Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c179fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69838b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into dependent and independent variables i.e, features and the target columns\n",
    "X=train['Text_data']\n",
    "Y=train['Label']\n",
    "#Splitting the data such that 33% will be used for testing and the remaining 67% will be used for training. \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,stratify=Y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e10d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47e870f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 5956/5956 [00:00<00:00, 67046.19it/s]\n"
     ]
    }
   ],
   "source": [
    "words_in_sentences=[]\n",
    "for i in tqdm(x_train):\n",
    "    words_in_sentences.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c85e9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=words_in_sentences, vector_size=200,workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5501fb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6026\n"
     ]
    }
   ],
   "source": [
    "vocab=list(model.wv.key_to_index.keys())\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d29b7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_w2vec(sentences):\n",
    "    transformed=[]\n",
    "    for sentence in tqdm(sentences):\n",
    "        count=0\n",
    "        vector=np.zeros(200)\n",
    "        for word in sentence.split():\n",
    "            if word in vocab:\n",
    "                vector+=model.wv.get_vector(word)\n",
    "                count+=1\n",
    "        if count!=0:\n",
    "            vector/=count\n",
    "            transformed.append(vector)\n",
    "        else:\n",
    "            print(sentence)\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a51d826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5956/5956 [00:22<00:00, 267.75it/s]\n",
      " 71%|██████████████████████████████████████████████████████▍                      | 2075/2935 [00:09<00:04, 201.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuckfuckfuckfuckfuck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2935/2935 [00:11<00:00, 266.20it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train_transformed=avg_w2vec(x_train)\n",
    "x_test_transformed=avg_w2vec(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "554d0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "grid_params = { 'n_neighbors' : [10,20,30,40,50,60],\n",
    "               'metric' : ['manhattan']}\n",
    "knn=KNeighborsClassifier()\n",
    "clf = RandomizedSearchCV(knn, grid_params, random_state=0,n_jobs=-1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1564bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5956"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_transformed.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1aeb950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5956"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22d18d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1154fe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5956"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87307e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=KNeighborsClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'metric': ['manhattan'],\n",
       "                                        'n_neighbors': [10, 20, 30, 40, 50,\n",
       "                                                        60]},\n",
       "                   random_state=0, verbose=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train_transformed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4976dbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6752854180402232"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31107043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1330\n",
      "           1       0.68      1.00      0.81      4022\n",
      "           2       0.00      0.00      0.00       604\n",
      "\n",
      "    accuracy                           0.68      5956\n",
      "   macro avg       0.23      0.33      0.27      5956\n",
      "weighted avg       0.46      0.68      0.54      5956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train,clf.predict(x_train_transformed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "084dd4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape[0]\n",
    "y_test=y_test[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4812a8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2934"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_transformed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b64506ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       655\n",
      "           1       0.68      1.00      0.81      1981\n",
      "           2       0.00      0.00      0.00       298\n",
      "\n",
      "    accuracy                           0.68      2934\n",
      "   macro avg       0.23      0.33      0.27      2934\n",
      "weighted avg       0.46      0.68      0.54      2934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,clf.predict(x_test_transformed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3dd77ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\shwetank verma\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "     -------------------------------------- 182.4/182.4 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\shwetank verma\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.24.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f16c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
